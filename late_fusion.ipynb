{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# CNN Building Tools below-these lines are causing problems in the code for some reason\n",
    "import keras\n",
    "from keras import layers, models\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from keras import backend as K\n",
    "\n",
    "# from images.ipynb import load_images\n",
    "from keras.backend import clear_session\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "df = pd.read_csv(\"HAM10000_metadata.csv\")\n",
    "df = df.sort_values(by=\"image_id\")\n",
    "lesion_type_dict = {\n",
    "    \"nv\": \"Melanocytic nevi\",\n",
    "    \"mel\": \"Melanoma\",\n",
    "    \"bkl\": \"Benign keratosis-like lesions\",\n",
    "    \"akiec\": \"Actinic keratoses\",\n",
    "    \"vasc\": \"Vascular lesions\",\n",
    "    \"df\": \"Dermatofibroma\",\n",
    "    \"bcc\": \"Basal Cell Carcinoma\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target variables for train test split\n",
    "# features = df[[\"dx_type\", \"age\", \"sex\", \"localization\", \"img\"]]\n",
    "features = np.load(\"images.npy\")\n",
    "target = df[[\"dx\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[229 132 134]\n",
      "[229 132 134]\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"HAM10000_images/ISIC_0024306.jpg\")\n",
    "img = np.array(img.resize((65,45)))\n",
    "print(img[0][0])\n",
    "print(features[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.get_dummies(target, columns=[\"dx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x_train = features[:6000]\n",
    "image_x_val = features[6000:7600]\n",
    "image_x_test = features[7600:]\n",
    "image_y_train = target[:6000]\n",
    "image_y_val = target[6000:7600]\n",
    "image_y_test = target[7600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of CNN Building\n",
    "input_shape = (45, 65, 3)\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 45, 65, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 45, 65, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 22, 32, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 22, 32, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 22, 32, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 11, 16, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 11, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 11, 16, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 11, 16, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 5, 8, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5, 8, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               163968    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 285831 (1.09 MB)\n",
      "Trainable params: 285831 (1.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialising the CNN\n",
    "model_cnn = Sequential()\n",
    "# Adding layers to the model\n",
    "model_cnn.add(layers.InputLayer(input_shape=input_shape))\n",
    "model_cnn.add(\n",
    "    Conv2D(\n",
    "        32,\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\",\n",
    "    )\n",
    ")\n",
    "\n",
    "model_cnn.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", padding=\"Same\"))\n",
    "model_cnn.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model_cnn.add(layers.Dropout(0.25))\n",
    "\n",
    "model_cnn.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"Same\"))\n",
    "model_cnn.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"Same\"))\n",
    "model_cnn.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model_cnn.add(layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "model_cnn.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"Same\"))\n",
    "model_cnn.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"Same\"))\n",
    "model_cnn.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model_cnn.add(layers.Dropout(0.25))\n",
    "\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(128, activation=\"relu\"))\n",
    "model_cnn.add(layers.Dropout(0.25))\n",
    "model_cnn.add(Dense(7))\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[[\"age\", \"localization\", \"sex\"]]\n",
    "text = pd.get_dummies(text, columns=[\"localization\", \"sex\"])\n",
    "text_x_train = np.array(text[:6000])\n",
    "text_x_test = np.array(text[6000:7600])\n",
    "text_x_val = np.array(text[7600:])\n",
    "text_y_train = np.array(target[:6000])\n",
    "text_y_test = np.array(target[6000:7600])\n",
    "text_y_val = np.array(target[7600:])\n",
    "# print(text_x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 15)                300       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 112       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 412 (1.61 KB)\n",
      "Trainable params: 412 (1.61 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_txt = Sequential()\n",
    "model_txt.add(layers.InputLayer(input_shape=(19,)))\n",
    "model_txt.add(Dense(15, activation=\"linear\"))\n",
    "model_txt.add(Dense(7))\n",
    "model_txt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text_side = model_txt(text_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([])\n",
    "# for i in range (0, len(image_x_train)):\n",
    "#     x_train[i] = text_train[i] + image_x_train[i]\n",
    "#     x_train = np.array([np.array([text_x_train[i], image_x_train[i]])])\n",
    "\n",
    "# x_train[0] = np.array([text_x_train[0], image_x_train[0]])\n",
    "\n",
    "x_train = [[text_x_train[i], image_x_train[i]] for i in range(0, len(image_x_train))]\n",
    "x_val = [[text_x_val[i], image_x_val[i]] for i in range(0, len(image_x_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "<class 'numpy.ndarray'>\n",
      "45\n",
      "(45, 65, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "tmp = x_train[0][1]\n",
    "print(len(x_train))\n",
    "print(type(x_train[0][0]))\n",
    "print(len(tmp))\n",
    "print(tmp.shape)\n",
    "print(type(tmp))\n",
    "# print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " img (InputLayer)            [(None, 45, 65, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " txt (InputLayer)            [(None, 19)]                 0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 7)                    285831    ['img[0][0]']                 \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)   (None, 7)                    412       ['txt[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 14)                   0         ['sequential[0][0]',          \n",
      "                                                                     'sequential_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 14)                   210       ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " class (Dense)               (None, 7)                    105       ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 286558 (1.09 MB)\n",
      "Trainable params: 286558 (1.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input = layers.Input(shape=input_shape, dtype=tf.int32, name=\"img\")\n",
    "txt_input = layers.Input(shape=(19,), dtype=tf.int32, name=\"txt\")\n",
    "image_side = model_cnn(img_input)\n",
    "text_side = model_txt(txt_input)\n",
    "\n",
    "merged = layers.Concatenate()([image_side, text_side])\n",
    "merged = layers.Dense(14, activation=\"relu\")(merged)\n",
    "output = layers.Dense(7, activation=\"softmax\", name=\"class\")(merged)\n",
    "model = models.Model([img_input, txt_input], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "# optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0005),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\", patience=3, verbose=1, factor=0.5, min_lr=0.00001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60/60 [==============================] - ETA: 0s - loss: 4.9762 - accuracy: 0.3473WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "60/60 [==============================] - 48s 789ms/step - loss: 4.9762 - accuracy: 0.3473 - lr: 5.0000e-04\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.6946 - accuracy: 0.4858WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "60/60 [==============================] - 42s 706ms/step - loss: 1.6946 - accuracy: 0.4858 - lr: 5.0000e-04\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.2375 - accuracy: 0.5985WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "60/60 [==============================] - 43s 722ms/step - loss: 1.2375 - accuracy: 0.5985 - lr: 5.0000e-04\n",
      "Epoch 4/5\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0996 - accuracy: 0.6415WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "60/60 [==============================] - 40s 666ms/step - loss: 1.0996 - accuracy: 0.6415 - lr: 5.0000e-04\n",
      "Epoch 5/5\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.0248 - accuracy: 0.6642 WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy,lr\n",
      "60/60 [==============================] - 845s 14s/step - loss: 1.0248 - accuracy: 0.6642 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 100\n",
    "history = model.fit(\n",
    "    [image_x_train, text_x_train],\n",
    "    text_y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    steps_per_epoch=6000 // batch_size,\n",
    "    callbacks=[learning_rate_reduction],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 7)\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(text_y_train.shape)\n",
    "print(len(x_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.]\n",
      "[0 0 0 0 0 1 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 45, 19\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_x_train[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_y_train[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict([image_x_train[\u001b[38;5;241m1\u001b[39m], text_x_train[\u001b[38;5;241m1\u001b[39m]])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1960\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1953\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1954\u001b[0m         label,\n\u001b[1;32m   1955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1956\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1957\u001b[0m         ),\n\u001b[1;32m   1958\u001b[0m     )\n\u001b[1;32m   1959\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1960\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 45, 19\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "print(text_x_train[1])\n",
    "print(text_y_train[1])\n",
    "model.predict([image_x_train[1], text_x_train[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
